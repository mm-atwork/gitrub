#!/usr/bin/python3
# -*- coding: utf-8 -*-
# pylint: disable=bare-except, too-many-locals, line-too-long

"""
gitrub - fetches a list of repos (or a single repo) from github to maintain and clean
"""

import argparse
from datetime import datetime, timezone
import json
import os
import sys
import shutil
from subprocess import PIPE, Popen
import requests
import boto3
from prettytable import PrettyTable

parser = argparse.ArgumentParser()
parser.add_argument("-r", "--repo", help="Search for a specific repo name, list all repos if unset")
parser.add_argument("-t", "--github-token", help="Github token required for authentication, set this or the environmental variable GH_TOKEN")
parser.add_argument("-o", "--organization", help="User organization to elaborate")
parser.add_argument("--archive", help="Backup a repository locally - Basically clone and zip, use --s3-bucket to upload to S3", action="store_true", default=False)
parser.add_argument("--s3-bucket", help="S3 bucket to upload to while using the --archive option")
parser.add_argument("-d", "--days", help="Set the number of days after of which a repo is considered 'stale'. Required to check old repos, if unset it won't fetch the repos' top contributor.", type=int)

args = parser.parse_args()

# To do: upload to S3 once archived

def main():

    """
    Once authenticated to github, the script is fetching
    a single repo or the full list of repos related one user
    or its organization.
    """

    github_token = ""

    if args.github_token:
        github_token = args.github_token
    else:
        if os.environ.get('GH_TOKEN'):
            github_token = os.getenv('GH_TOKEN')
        else:
            print("No -t/--github-token arguments passed or GH_TOKEN environment variable found. Please pass the GitHub token by setting the argument or the environment variable")
            sys.exit(1)

    headers = {
        "Authorization": "token " + github_token,
        "Accept": "application/vnd.github+json"
    }

    api_baseurl = "https://api.github.com/"

    # Check Github API rates before continuing
    rates = (requests.get(api_baseurl + "rate_limit", headers=headers, timeout=10)).json()
    try:
        remaining = rates["resources"]["core"]["remaining"]
        reset = datetime.fromtimestamp(rates["resources"]["core"]["reset"]).strftime('%c')
        if remaining < 20:
            print(f"WARNING: only {str(remaining)} remaining API calls, will reset at {str(reset)}")
        elif remaining == 0:
            print(f"ERROR: no API calls last, will reset at {str(reset)}")
            sys.exit(1)
    except:
        print("Error while checking current API call rates. Check your authentication is still valid")
        sys.exit(1)

    repos = []
    user = (requests.get(api_baseurl + "user", headers=headers, timeout=10)).json()["login"]

    if args.repo:

        if args.organization:
            api_url = api_baseurl + 'repos/' + args.organization + '/' + args.repo
        else:
            api_url = api_baseurl + 'repos/' + user + '/' + args.repo

        repo = (requests.get(api_url, headers=headers, timeout=10)).json()

        if "message" in repo:
            if repo["message"] == "Not Found":
                print("Repository not found, please check if it actually exists")
        else:
            last_update = get_last_update(repo)
            updated = ""
            repo['description'] = ""
            repo['stale'] = "False"
            main_contributor = ""

            if last_update:
                updated = last_update[0]
                if args.days:
                    if last_update[1] > args.days:
                        repo['stale'] = "True"
            else:
                updated = "NotAvailable"

            if repo['description']:
                repo['description'] = (repo['description'][:50] + '..') if len(repo['description']) > 50 else repo['description']

            if args.days:
                main_contributor = get_contrib(repo, headers)

            if args.archive:
                git_archive(repo, github_token, args.s3_bucket)

            exit()

            repos.append(repo['name'] + '$' + repo['description'] + '$' + updated + "$" + main_contributor + "$" + repo['stale'])
    else:

        params = {'per_page' : 100}
        another_page = True

        if args.organization:
            api_url = api_baseurl + 'orgs/' + args.organization + '/repos'
        else:
            api_url = api_baseurl + 'user/repos'
            params['type'] = "owner"

        orgs_repos = []

        while another_page:
            github_response = requests.get(api_url, params=params, headers=headers, timeout=10)
            json_response = json.loads(github_response.text)
            orgs_repos.append(json_response)

            if 'next' in github_response.links:
                api_url = github_response.links['next']['url']
            else:
                another_page=False

        for page in orgs_repos:
            for repo in page:
                last_update = get_last_update(repo)
                repo['stale'] = "False"
                if last_update:
                    updated = last_update[0]
                    if args.days:
                        if last_update[1] > args.days:
                            repo['stale'] = "True"
                else:
                    updated = "NotAvailable"

                # Truncate if description is too long
                if repo['description']:
                    repo['description'] = (repo['description'][:80] + '..') if len(repo['description']) > 80 else repo['description']
                else:
                    repo['description'] = ""

                # Actions for stale repos
                main_contributor = ""
                if repo['stale'] == "True":
                    main_contributor = get_contrib(repo, headers)

                repos.append(repo['name'] + '$' + repo['description'] + '$' + updated + "$" + main_contributor + "$" + repo['stale'])

    pt_headers = ["Repo", "Description", "LastPush", "MainContributor", "Stale"]

    pt_table = PrettyTable(pt_headers)
    pt_table.align = "l"

    for repo_det in repos:
        pt_table.add_row(repo_det.split("$"))

    print(pt_table.get_string(sortby="LastPush"))


def get_last_update(repo):

    """
    From the repo object returned from an github API call
    it returns a list with the latest update to the repo
    and the number of days from it as an integer.
    The date returned is the most recent evaluated between
    the values pushed_at and updated_at
    """

    try:
        last_push = datetime.strptime(repo['pushed_at'], '%Y-%m-%dT%H:%M:%SZ')
    except:
        last_push = None

    try:
        last_update = datetime.strptime(repo['updated_at'], '%Y-%m-%dT%H:%M:%SZ')
    except:
        last_update = None

    if last_push and last_update:

        last_change = last_update

        if last_push > last_update:
            last_change = last_push

        stale_repo_days = (datetime.now(timezone.utc) - last_change.replace(tzinfo=timezone.utc)).days

        return [last_change.strftime('%Y-%m-%d %H:%M'), stale_repo_days]

    return ["-", None]


def get_contrib(repo, headers):

    """
    From the repo object returned from an github API call
    it returns the github username with the highest number
    of updates to the repo.
    """

    params = {'per_page': 1}

    try:
        api_url = "https://api.github.com/repos/" + repo["full_name"] + "/contributors"
        contrib = (requests.get(api_url, params=params, headers=headers, timeout=10)).json()
        return contrib[0]['login']
    except:
        return "notfound"


def git_archive(repo, github_token, destination_bucket):

    """
    Clone a git repo, zip it and delete the clone folder
    """

    # Set the URL to use the GitHub token to clone
    repo_url = repo["clone_url"].replace("https://", f"https://{github_token}@")

    # Clone the repo into a folder with the same name
    with Popen("git clone " + repo_url + " " + repo["name"], stdout=PIPE, stderr=PIPE, shell=True) as process:
        process.communicate()[0].decode("utf-8")

    # ZIP the whole folder (zipfile, 'zip', folder_to_archive)
    shutil.make_archive(repo["name"], 'zip', repo["name"])

    if destination_bucket:
        # Upload the zip file on S3
        s3_res = boto3.resource('s3')
        s3_res.Bucket(destination_bucket).upload_file(f"{repo['name']}.zip", f"github_backup/{repo['name']}.zip")

    # Delete the zip file and the cloned directory
    shutil.rmtree(repo["name"])


if __name__ == '__main__':
    main()
